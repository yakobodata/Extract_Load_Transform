{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engineering about Movies\n",
    "In this project we are preparing data about movies and serve it to Big Query . We get this data about the likability , length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Python Libraries\n",
    "Firstly we are going to install a couple of python libraries that will help us to execute the Extract Transform and Load Process . What we do is that we list all the Libraries in a file called 'requirements.txt'\n",
    "the libraries include."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pandas\n",
    "numpy\n",
    "mysql.connector\n",
    "google-cloud\n",
    "pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install the above libraries , we run a python command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 1)) (2.0.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 2)) (1.24.3)\n",
      "Requirement already satisfied: mysql.connector in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 3)) (2.2.9)\n",
      "Requirement already satisfied: google-cloud in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 4)) (0.34.0)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from -r requirements.txt (line 5)) (11.0.0)\n",
      "Collecting google-cloud-bigquery (from -r requirements.txt (line 6))\n",
      "  Obtaining dependency information for google-cloud-bigquery from https://files.pythonhosted.org/packages/1e/5a/69eef5c1d2c99db33ac7f1049b26799587b2a6d5c3980091d4ecef1cbe5c/google_cloud_bigquery-3.16.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_cloud_bigquery-3.16.0-py2.py3-none-any.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from pandas->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from pandas->-r requirements.txt (line 1)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from pandas->-r requirements.txt (line 1)) (2023.3)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from google-cloud-bigquery->-r requirements.txt (line 6)) (2.15.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from google-cloud-bigquery->-r requirements.txt (line 6)) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from google-cloud-bigquery->-r requirements.txt (line 6)) (2.7.0)\n",
      "Requirement already satisfied: packaging>=20.0.0 in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from google-cloud-bigquery->-r requirements.txt (line 6)) (23.1)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.21.0 in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from google-cloud-bigquery->-r requirements.txt (line 6)) (2.31.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery->-r requirements.txt (line 6)) (1.62.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery->-r requirements.txt (line 6)) (4.21.12)\n",
      "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery->-r requirements.txt (line 6)) (2.25.2)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery->-r requirements.txt (line 6)) (1.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 1)) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery->-r requirements.txt (line 6)) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery->-r requirements.txt (line 6)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery->-r requirements.txt (line 6)) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery->-r requirements.txt (line 6)) (2023.11.17)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery->-r requirements.txt (line 6)) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery->-r requirements.txt (line 6)) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery->-r requirements.txt (line 6)) (4.9)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\jacob\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-bigquery->-r requirements.txt (line 6)) (0.4.8)\n",
      "Downloading google_cloud_bigquery-3.16.0-py2.py3-none-any.whl (229 kB)\n",
      "   ---------------------------------------- 0.0/229.9 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/229.9 kB ? eta -:--:--\n",
      "   ----- --------------------------------- 30.7/229.9 kB 435.7 kB/s eta 0:00:01\n",
      "   ---------- ---------------------------- 61.4/229.9 kB 544.7 kB/s eta 0:00:01\n",
      "   ------------------------- ------------ 153.6/229.9 kB 919.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------  225.3/229.9 kB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 229.9/229.9 kB 1.0 MB/s eta 0:00:00\n",
      "Installing collected packages: google-cloud-bigquery\n",
      "Successfully installed google-cloud-bigquery-3.16.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Transform Load Overview \n",
    "\n",
    "## Extract \n",
    "We are going to extract data from MYSQL database , We do this to get data from the production environment into an analytical environment. We shall be extracting our data from an SQL database in this project\n",
    "\n",
    "## Transform\n",
    "This step is optional but this is the step where by we get to add columns , change data types of data columns . This is done according to business rules.\n",
    "\n",
    "## Load\n",
    "This is the step where we load our data into an analytical environments , there are different tools that are used to achieve this but we are going to use Google Big query in this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Python using MYSQL\n",
    "\n",
    "What we do is that we put credentials that allow access to an online mysql database in file called credentials.py which you won't see here because i have instructed git to ignore it . I do this as a security measure not to reveal the keys to the production database.\n",
    "\n",
    "\n",
    "Next i created a file called connections.py . This file will house all our connections to avoid repetition of code.\n",
    "\n",
    "The code below is what included in the file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to MySQL Server version 5.7.23-23\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "# import create_customers\n",
    "import credentials\n",
    "# Replace these variables with your actual database credentials\n",
    "\n",
    "\n",
    "db_config = {\n",
    "    'user': credentials.user[0],\n",
    "    'password': credentials.password[0],\n",
    "    'host': credentials.host[0],\n",
    "    'database': credentials.database[0],\n",
    "}\n",
    "\n",
    "\n",
    "# Establish a connection to the database\n",
    "connection = mysql.connector.connect(**db_config)\n",
    "try:\n",
    "    # Establish a connection to the database\n",
    "    connection = mysql.connector.connect(**db_config)\n",
    "    if connection.is_connected():\n",
    "        db_info = connection.get_server_info()\n",
    "        print(f\"Connected to MySQL Server version {db_info}\")\n",
    "\n",
    "except mysql.connector.Error as e:\n",
    "    print(f\"Error connecting to MySQL: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Data Types \n",
    "The next thing that we are doing is to do pandas data types , we are doing this inorder to check the types of data that we do have in our data such that we faciliate further transformations like mathematical operations in accordance to business rules. One has to be sure of the types of data that they are dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jacob\\AppData\\Local\\Temp\\ipykernel_30496\\1593940848.py:9: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query,connection)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year                                              title  \\\n",
      "0  1906                        The Story of the Kelly Gang   \n",
      "1  1911                                     Den sorte drøm   \n",
      "2  1912                                          Cleopatra   \n",
      "3  1911                                          L'Inferno   \n",
      "4  1912  From the Manger to the Cross; or, Jesus of Naz...   \n",
      "\n",
      "                       genre  \n",
      "0    Biography, Crime, Drama  \n",
      "1                      Drama  \n",
      "2             Drama, History  \n",
      "3  Adventure, Drama, Fantasy  \n",
      "4           Biography, Drama  \n",
      "year      int64\n",
      "title    object\n",
      "genre    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "query = \"select year \" \\\n",
    "        \", title \" \\\n",
    "        \", genre \"  \\\n",
    "        \"from `oscarval_sql_course`.`imdb_movies` \" \\\n",
    "        \"limit 7\"\n",
    "\n",
    "df = pd.read_sql(query,connection)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "print(df.dtypes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas filters\n",
    "\n",
    "This enables us to filter out information that is required to fulfill the business rule , for example in this instance we need to see the movies that were established in 2005\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jacob\\AppData\\Local\\Temp\\ipykernel_42616\\2153758935.py:9: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, connection)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year                                         title  \\\n",
      "0  2005                             The Naked Monster   \n",
      "1  2005                                   What Is It?   \n",
      "2  2005                                Fantastic Four   \n",
      "3  2005                                  Corpse Bride   \n",
      "4  2005  Star Wars: Episode III - Revenge of the Sith   \n",
      "\n",
      "                        genre  avg_vote  \n",
      "0      Comedy, Horror, Sci-Fi       5.6  \n",
      "1                       Drama       5.6  \n",
      "2  Action, Adventure, Fantasy       5.7  \n",
      "3    Animation, Drama, Family       7.3  \n",
      "4  Action, Adventure, Fantasy       7.5  \n"
     ]
    }
   ],
   "source": [
    "query = \"select year \" \\\n",
    "        \",  title \" \\\n",
    "        \",  genre \" \\\n",
    "        \",  avg_vote \" \\\n",
    "        \"from `oscarval_sql_course`.`imdb_movies` \" \\\n",
    "        \"where year between 2005 and 2006\"\n",
    "        \n",
    "\n",
    "df = pd.read_sql(query, connection)\n",
    "\n",
    "yr_2005 = df['year'] == 2005\n",
    "\n",
    "print(df[yr_2005].head()) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Export to CSV\n",
    "What we are now doing is that we are extracting the data from mysql into pandas then to csv , We do this to prepare our data to be ingested in the Cloud warehouse Big query. In this cell we want to see the movies that were released in 2005."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jacob\\code\\portifolio_projects\\Extract_Load_Transform\\data_files\\movies.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jacob\\AppData\\Local\\Temp\\ipykernel_30496\\736300270.py:17: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, connection)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "cur_path = os.getcwd()\n",
    "\n",
    "file = 'movies.csv'\n",
    "\n",
    "file_path = os.path.join(cur_path,'data_files',file)\n",
    "\n",
    "print(file_path)\n",
    "query = \"select year \"\\\n",
    "        \",  title \"\\\n",
    "        \",  genre \"\\\n",
    "        \",  avg_vote \"\\\n",
    "        \"from `oscarval_sql_course`.`imdb_movies` \"\\\n",
    "        \"where year between 2005 and 2006\" \n",
    "\n",
    "df = pd.read_sql(query, connection)\n",
    "\n",
    "yr_2005 = df['year'] == 2005\n",
    "\n",
    "df[yr_2005].to_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas new Column using SQL\n",
    "\n",
    "Here we are going to grade the movies and see if they were good movies or bad movies according the average votes that they received"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jacob\\code\\portifolio_projects\\Extract_Load_Transform\\data_files\\movie_rating.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jacob\\AppData\\Local\\Temp\\ipykernel_30496\\3021959094.py:23: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, connection)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "cur_path = os.getcwd()\n",
    "\n",
    "file = 'movie_rating.csv'\n",
    "\n",
    "file_path = os.path.join(cur_path,'data_files',file)\n",
    "\n",
    "print(file_path)\n",
    "query = \"select year \"\\\n",
    "        \",  title \"\\\n",
    "        \",  genre \"\\\n",
    "        \",  avg_vote \"\\\n",
    "        \",  case \"\\\n",
    "        \" when avg_vote < 3 then 'bad' \"\\\n",
    "        \" WHEN avg_vote BETWEEN 3 AND 6 THEN 'okay' \"\\\n",
    "        \" when avg_vote >= 6 then 'good' \"\\\n",
    "        \" end as movie_rating \" \\\n",
    "        \" from `oscarval_sql_course`.`imdb_movies` \"\\\n",
    "        \"where year between 2005 and 2006\" \n",
    "\n",
    "df = pd.read_sql(query, connection)\n",
    "\n",
    "yr_2005 = df['year'] == 2005\n",
    "\n",
    "df[yr_2005].to_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas new Column with Python Function\n",
    "\n",
    "Here we are going to look at length of the movies and categorize them and see if they were really short , average and really long .Essentially here we are looking to see the watchability of the movie . Some people like long movies others like short movies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jacob\\code\\portifolio_projects\\Extract_Load_Transform\\data_files\\watchability.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jacob\\AppData\\Local\\Temp\\ipykernel_30496\\4284993103.py:35: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, connection)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "cur_path = os.getcwd()\n",
    "\n",
    "file = 'watchability.csv'\n",
    "\n",
    "file_path = os.path.join(cur_path,'data_files',file)\n",
    "\n",
    "print(file_path)\n",
    "query = \"select year \"\\\n",
    "        \",  title \"\\\n",
    "        \",  genre \"\\\n",
    "        \",  avg_vote \"\\\n",
    "        \",  case \"\\\n",
    "        \" when avg_vote < 3 then 'bad' \"\\\n",
    "        \" WHEN avg_vote BETWEEN 3 AND 6 THEN 'okay' \"\\\n",
    "        \" when avg_vote >= 6 then 'good' \"\\\n",
    "        \" end as movie_rating \" \\\n",
    "        \", duration \" \\\n",
    "        \" from `oscarval_sql_course`.`imdb_movies` \"\\\n",
    "        \"where year between 2005 and 2006\" \n",
    "\n",
    "# create duration label function\n",
    "def movie_duration(d):\n",
    "    if d < 60:\n",
    "        return 'short movie'\n",
    "    elif d > 60 and d < 90:\n",
    "        return 'avg length movie'\n",
    "    elif d > 90 and d < 5000:\n",
    "        return 'really long movie'\n",
    "    else:\n",
    "        return 'no data'\n",
    "    \n",
    "df = pd.read_sql(query, connection)\n",
    "\n",
    "df['watchability'] = df['duration'].apply(movie_duration)\n",
    "\n",
    "df.to_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data to Big Query\n",
    "We are now loading data to big query using python , this is the start of the final step in the ETL process .We are looking at the movie_rating , movies and watchability of the movies in the Big query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 1806 rows in your table \n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "import os\n",
    "\n",
    "client = bigquery.Client(project='charming-autumn-407214')\n",
    "target_table_1 = 'charming-autumn-407214.sample_dataset.movies'\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    skip_leading_rows = 1,\n",
    "    source_format = bigquery.SourceFormat.CSV,\n",
    "    autodetect=True\n",
    ")\n",
    "\n",
    "# file vars\n",
    "cur_path = os.getcwd()\n",
    "file = 'movies.csv'\n",
    "file_path = os.path.join(cur_path,'data_files',file)\n",
    "\n",
    "with open(file_path, 'rb') as source_file:\n",
    "    load_job   = client.load_table_from_file(\n",
    "        source_file,\n",
    "        target_table_1,\n",
    "        job_config=job_config\n",
    "\n",
    "    )\n",
    "\n",
    "load_job.result()\n",
    "\n",
    "destination_table = client.get_table(target_table_1)\n",
    "\n",
    "print(f\"You have {destination_table.num_rows} rows in your table \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 3851 rows in your table \n"
     ]
    }
   ],
   "source": [
    "#watchability\n",
    "from google.cloud import bigquery\n",
    "import os\n",
    "\n",
    "client = bigquery.Client(project='charming-autumn-407214')\n",
    "target_table_2 = 'charming-autumn-407214.sample_dataset.watchability'\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    skip_leading_rows = 1,\n",
    "    source_format = bigquery.SourceFormat.CSV,\n",
    "    autodetect=True\n",
    ")\n",
    "\n",
    "# file vars\n",
    "cur_path = os.getcwd()\n",
    "file = 'watchability.csv'\n",
    "file_path = os.path.join(cur_path,'data_files',file)\n",
    "\n",
    "with open(file_path, 'rb') as source_file:\n",
    "    load_job   = client.load_table_from_file(\n",
    "        source_file,\n",
    "        target_table_2,\n",
    "        job_config=job_config\n",
    "\n",
    "    )\n",
    "\n",
    "load_job.result()\n",
    "\n",
    "destination_table = client.get_table(target_table_2)\n",
    "\n",
    "print(f\"You have {destination_table.num_rows} rows in your table \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 3851 rows in your table \n"
     ]
    }
   ],
   "source": [
    "#movie_rating\n",
    "from google.cloud import bigquery\n",
    "import os\n",
    "\n",
    "client = bigquery.Client(project='charming-autumn-407214')\n",
    "target_table_2 = 'charming-autumn-407214.sample_dataset.movie_rating'\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    skip_leading_rows = 1,\n",
    "    source_format = bigquery.SourceFormat.CSV,\n",
    "    autodetect=True\n",
    ")\n",
    "\n",
    "# file vars\n",
    "cur_path = os.getcwd()\n",
    "file = 'movie_rating.csv'\n",
    "file_path = os.path.join(cur_path,'data_files',file)\n",
    "\n",
    "with open(file_path, 'rb') as source_file:\n",
    "    load_job   = client.load_table_from_file(\n",
    "        source_file,\n",
    "        target_table_2,\n",
    "        job_config=job_config\n",
    "\n",
    "    )\n",
    "\n",
    "load_job.result()\n",
    "\n",
    "destination_table = client.get_table(target_table_2)\n",
    "\n",
    "print(f\"You have {destination_table.num_rows} rows in your table \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
