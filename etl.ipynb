{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engineering about Movies üöÄ\n",
    "I'm excited to share the progress in my data engineering studies! üöÄ\n",
    "In this project we are preparing data about movies and serving it to a scalable Storage called Big Query . The data is about the likability , length of movies .\n",
    "\n",
    "\n",
    "In this repository, I explore one of the many possible options for implementing an ETL pipeline.\n",
    "\n",
    "- I'm using two types of data sources, a local one represented by a MySQL database, and a remote one represented by a Public API. üèõÔ∏èüåê\n",
    "- In the two working notebooks (API_ETL.ipynb, etl.ipynb), I explore two ways of transforming data. The first method uses a Pandas transformation in a local environment, and the second involves executing a data extraction using an API with the necessary transformations to add business context to the data making it ready for analysis. üìäüîÑ\n",
    "- Finally, using Python libraries for Google Cloud Platform, I store these analysis-ready data in BigQuery, completing the ETL process. üêçüîçüìä"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ETL DIAGRAM](./images/Movies_2024.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Python Libraries\n",
    "Firstly we are going to install a couple of python libraries that will help us to execute the Extract Transform and Load Process . What we do is that we list all the Libraries in a file called 'requirements.txt'\n",
    "the libraries include."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pandas\n",
    "numpy\n",
    "mysql.connector\n",
    "google-cloud\n",
    "pyarrow\n",
    "requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install the above libraries , we run a python command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Transform Load Overview \n",
    "\n",
    "## Extract \n",
    "We are going to extract data from MYSQL database , We do this to get data from the production environment into an analytical environment we do this ensure optimized performance of the production database. We shall be extracting our data from an SQL database in this project\n",
    "\n",
    "## Transform\n",
    "This step is optional but this is the step where by we get to add columns , change data types of data columns . This is done according to business rules.\n",
    "\n",
    "## Load\n",
    "This is the step where we load our data into analytical environments , there are different tools that are used to achieve this but we are going to use Google Big query(Scalable Storage) in this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Python using MYSQL\n",
    "\n",
    "What we do is that we put credentials that allow access to an online mysql database in file called credentials.py which you won't see here because i have instructed git to ignore it . I do this as a security measure not to reveal the keys to the production database.\n",
    "\n",
    "\n",
    "Next i write code for connection to the mysql database . I only write this code here once to avoid repetition of code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to MySQL Server version 5.7.23-23\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "# import create_customers\n",
    "import credentials\n",
    "# Replace these variables with your actual database credentials\n",
    "\n",
    "\n",
    "db_config = {\n",
    "    'user': credentials.user[0],\n",
    "    'password': credentials.password[0],\n",
    "    'host': credentials.host[0],\n",
    "    'database': credentials.database[0],\n",
    "}\n",
    "\n",
    "\n",
    "# Establish a connection to the database\n",
    "connection = mysql.connector.connect(**db_config)\n",
    "try:\n",
    "    # Establish a connection to the database\n",
    "    connection = mysql.connector.connect(**db_config)\n",
    "    if connection.is_connected():\n",
    "        db_info = connection.get_server_info()\n",
    "        print(f\"Connected to MySQL Server version {db_info}\")\n",
    "\n",
    "except mysql.connector.Error as e:\n",
    "    print(f\"Error connecting to MySQL: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Data Types \n",
    "The next thing that we are doing is to do is the check the data types of the columns in our data using pandas data types , we are doing this inorder to check the types of data that we do have in our data such that we faciliate further transformations like mathematical operations in accordance to business rules. One has to be sure of the types of data that they are dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jacob\\AppData\\Local\\Temp\\ipykernel_30496\\1593940848.py:9: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query,connection)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year                                              title  \\\n",
      "0  1906                        The Story of the Kelly Gang   \n",
      "1  1911                                     Den sorte dr√∏m   \n",
      "2  1912                                          Cleopatra   \n",
      "3  1911                                          L'Inferno   \n",
      "4  1912  From the Manger to the Cross; or, Jesus of Naz...   \n",
      "\n",
      "                       genre  \n",
      "0    Biography, Crime, Drama  \n",
      "1                      Drama  \n",
      "2             Drama, History  \n",
      "3  Adventure, Drama, Fantasy  \n",
      "4           Biography, Drama  \n",
      "year      int64\n",
      "title    object\n",
      "genre    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "query = \"select year \" \\\n",
    "        \", title \" \\\n",
    "        \", genre \"  \\\n",
    "        \"from `oscarval_sql_course`.`imdb_movies` \" \\\n",
    "        \"limit 7\"\n",
    "\n",
    "df = pd.read_sql(query,connection)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "print(df.dtypes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas filters\n",
    "\n",
    "This enables us to filter out information that is required to fulfill the business rule , for example in this instance we need to see the movies that were established in 2005\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jacob\\AppData\\Local\\Temp\\ipykernel_42616\\2153758935.py:9: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, connection)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year                                         title  \\\n",
      "0  2005                             The Naked Monster   \n",
      "1  2005                                   What Is It?   \n",
      "2  2005                                Fantastic Four   \n",
      "3  2005                                  Corpse Bride   \n",
      "4  2005  Star Wars: Episode III - Revenge of the Sith   \n",
      "\n",
      "                        genre  avg_vote  \n",
      "0      Comedy, Horror, Sci-Fi       5.6  \n",
      "1                       Drama       5.6  \n",
      "2  Action, Adventure, Fantasy       5.7  \n",
      "3    Animation, Drama, Family       7.3  \n",
      "4  Action, Adventure, Fantasy       7.5  \n"
     ]
    }
   ],
   "source": [
    "query = \"select year \" \\\n",
    "        \",  title \" \\\n",
    "        \",  genre \" \\\n",
    "        \",  avg_vote \" \\\n",
    "        \"from `oscarval_sql_course`.`imdb_movies` \" \\\n",
    "        \"where year between 2005 and 2006\"\n",
    "        \n",
    "\n",
    "df = pd.read_sql(query, connection)\n",
    "\n",
    "yr_2005 = df['year'] == 2005\n",
    "\n",
    "print(df[yr_2005].head()) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Export to CSV\n",
    "What we are now doing is that we are extracting the data from mysql into pandas then to csv , We do this to prepare our data to be ingested in the Cloud warehouse Big query. In this cell we want to see the movies that were released in 2005 being stored in csv waiting to be moved to a scalable storage like Google Big Query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jacob\\code\\portifolio_projects\\Extract_Load_Transform\\data_files\\movies.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jacob\\AppData\\Local\\Temp\\ipykernel_30496\\736300270.py:17: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, connection)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "cur_path = os.getcwd()\n",
    "\n",
    "file = 'movies.csv'\n",
    "\n",
    "file_path = os.path.join(cur_path,'data_files',file)\n",
    "\n",
    "print(file_path)\n",
    "query = \"select year \"\\\n",
    "        \",  title \"\\\n",
    "        \",  genre \"\\\n",
    "        \",  avg_vote \"\\\n",
    "        \"from `oscarval_sql_course`.`imdb_movies` \"\\\n",
    "        \"where year between 2005 and 2006\" \n",
    "\n",
    "df = pd.read_sql(query, connection)\n",
    "\n",
    "yr_2005 = df['year'] == 2005\n",
    "\n",
    "df[yr_2005].to_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas new Column using SQL\n",
    "\n",
    "Here we are going to grade the movies and see if they were good movies or bad movies according the average votes that they received"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jacob\\code\\portifolio_projects\\Extract_Load_Transform\\data_files\\movie_rating.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jacob\\AppData\\Local\\Temp\\ipykernel_30496\\3021959094.py:23: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, connection)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "cur_path = os.getcwd()\n",
    "\n",
    "file = 'movie_rating.csv'\n",
    "\n",
    "file_path = os.path.join(cur_path,'data_files',file)\n",
    "\n",
    "print(file_path)\n",
    "query = \"select year \"\\\n",
    "        \",  title \"\\\n",
    "        \",  genre \"\\\n",
    "        \",  avg_vote \"\\\n",
    "        \",  case \"\\\n",
    "        \" when avg_vote < 3 then 'bad' \"\\\n",
    "        \" WHEN avg_vote BETWEEN 3 AND 6 THEN 'okay' \"\\\n",
    "        \" when avg_vote >= 6 then 'good' \"\\\n",
    "        \" end as movie_rating \" \\\n",
    "        \" from `oscarval_sql_course`.`imdb_movies` \"\\\n",
    "        \"where year between 2005 and 2006\" \n",
    "\n",
    "df = pd.read_sql(query, connection)\n",
    "\n",
    "yr_2005 = df['year'] == 2005\n",
    "\n",
    "df[yr_2005].to_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas new Column with Python Function\n",
    "\n",
    "Here we are going to look at length of the movies and categorize them and see if they were really short , average and really long .Essentially here we are looking to see the watchability of the movie . Some people like long movies others like short movies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jacob\\code\\portifolio_projects\\Extract_Load_Transform\\data_files\\watchability.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jacob\\AppData\\Local\\Temp\\ipykernel_30496\\4284993103.py:35: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, connection)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "cur_path = os.getcwd()\n",
    "\n",
    "file = 'watchability.csv'\n",
    "\n",
    "file_path = os.path.join(cur_path,'data_files',file)\n",
    "\n",
    "print(file_path)\n",
    "query = \"select year \"\\\n",
    "        \",  title \"\\\n",
    "        \",  genre \"\\\n",
    "        \",  avg_vote \"\\\n",
    "        \",  case \"\\\n",
    "        \" when avg_vote < 3 then 'bad' \"\\\n",
    "        \" WHEN avg_vote BETWEEN 3 AND 6 THEN 'okay' \"\\\n",
    "        \" when avg_vote >= 6 then 'good' \"\\\n",
    "        \" end as movie_rating \" \\\n",
    "        \", duration \" \\\n",
    "        \" from `oscarval_sql_course`.`imdb_movies` \"\\\n",
    "        \"where year between 2005 and 2006\" \n",
    "\n",
    "# create duration label function\n",
    "def movie_duration(d):\n",
    "    if d < 60:\n",
    "        return 'short movie'\n",
    "    elif d > 60 and d < 90:\n",
    "        return 'avg length movie'\n",
    "    elif d > 90 and d < 5000:\n",
    "        return 'really long movie'\n",
    "    else:\n",
    "        return 'no data'\n",
    "    \n",
    "df = pd.read_sql(query, connection)\n",
    "\n",
    "df['watchability'] = df['duration'].apply(movie_duration)\n",
    "\n",
    "df.to_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data to Big Query\n",
    "We are now loading data to big query using python , this is the start of the final step in the ETL process .We are looking at moving the movie_rating , movies and watchability data of the movies in the Big query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 1806 rows in your table \n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "import os\n",
    "\n",
    "client = bigquery.Client(project='charming-autumn-407214')\n",
    "target_table_1 = 'charming-autumn-407214.sample_dataset.movies'\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    skip_leading_rows = 1,\n",
    "    source_format = bigquery.SourceFormat.CSV,\n",
    "    autodetect=True\n",
    ")\n",
    "\n",
    "# file vars\n",
    "cur_path = os.getcwd()\n",
    "file = 'movies.csv'\n",
    "file_path = os.path.join(cur_path,'data_files',file)\n",
    "\n",
    "with open(file_path, 'rb') as source_file:\n",
    "    load_job   = client.load_table_from_file(\n",
    "        source_file,\n",
    "        target_table_1,\n",
    "        job_config=job_config\n",
    "\n",
    "    )\n",
    "\n",
    "load_job.result()\n",
    "\n",
    "destination_table = client.get_table(target_table_1)\n",
    "\n",
    "print(f\"You have {destination_table.num_rows} rows in your table \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 3851 rows in your table \n"
     ]
    }
   ],
   "source": [
    "#watchability\n",
    "from google.cloud import bigquery\n",
    "import os\n",
    "\n",
    "client = bigquery.Client(project='charming-autumn-407214')\n",
    "target_table_2 = 'charming-autumn-407214.sample_dataset.watchability'\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    skip_leading_rows = 1,\n",
    "    source_format = bigquery.SourceFormat.CSV,\n",
    "    autodetect=True\n",
    ")\n",
    "\n",
    "# file vars\n",
    "cur_path = os.getcwd()\n",
    "file = 'watchability.csv'\n",
    "file_path = os.path.join(cur_path,'data_files',file)\n",
    "\n",
    "with open(file_path, 'rb') as source_file:\n",
    "    load_job   = client.load_table_from_file(\n",
    "        source_file,\n",
    "        target_table_2,\n",
    "        job_config=job_config\n",
    "\n",
    "    )\n",
    "\n",
    "load_job.result()\n",
    "\n",
    "destination_table = client.get_table(target_table_2)\n",
    "\n",
    "print(f\"You have {destination_table.num_rows} rows in your table \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 5657 rows in your table \n"
     ]
    }
   ],
   "source": [
    "#movie_rating\n",
    "from google.cloud import bigquery\n",
    "import os\n",
    "\n",
    "client = bigquery.Client(project='charming-autumn-407214')\n",
    "target_table_2 = 'charming-autumn-407214.sample_dataset.movie_rating'\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    skip_leading_rows = 1,\n",
    "    source_format = bigquery.SourceFormat.CSV,\n",
    "    autodetect=True\n",
    ")\n",
    "\n",
    "# file vars\n",
    "cur_path = os.getcwd()\n",
    "file = 'movie_rating.csv'\n",
    "file_path = os.path.join(cur_path,'data_files',file)\n",
    "\n",
    "with open(file_path, 'rb') as source_file:\n",
    "    load_job   = client.load_table_from_file(\n",
    "        source_file,\n",
    "        target_table_2,\n",
    "        job_config=job_config\n",
    "\n",
    "    )\n",
    "load_job.result()\n",
    "\n",
    "destination_table = client.get_table(target_table_2)\n",
    "\n",
    "print(f\"You have {destination_table.num_rows} rows in your table \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
